fname <- paste0("../data/wikipagesEditStatistics/", articles_names[i], ".html")
if (!file.exists(fname)) {
try(download.file(url, destfile = fname))
Sys.sleep(runif(1, 0, 1))
}
if (i%%100==0) { print(paste0(i, " downloads"))}
}
options(timeout=10) # set timeout of 10sec
for (i in rev(seq_along(articles_names))) {
url <- paste0("https://tools.wmflabs.org/xtools-articleinfo/?article=", articles_names[i], "&project=en.wikipedia.org")
fname <- paste0("../data/wikipagesEditStatistics/", articles_names[i], ".html")
if (!file.exists(fname)) {
try(download.file(url, destfile = fname))
Sys.sleep(runif(1, 0, 1))
}
if (i%%100==0) { print(paste0(i, " downloads"))}
}
options(timeout=10) # set timeout of 10sec
for (i in rev(seq_along(articles_names))) {
url <- paste0("https://tools.wmflabs.org/xtools-articleinfo/?article=", articles_names[i], "&project=en.wikipedia.org")
fname <- paste0("../data/wikipagesEditStatistics/", articles_names[i], ".html")
if (!file.exists(fname)) {
try(download.file(url, destfile = fname))
Sys.sleep(runif(1, 0, 1))
}
if (i%%100==0) { print(paste0(i, " downloads"))}
}
## load packages and functions -------------------------------
source("_packages.r")
source("_functions.r")
## load article names ----------------------------------------
load("../data/articlesHTML.RData")
library(doMC)
install.packages("doMC")
library(doMC)
detectCores()
registerDoMC(3)
getDoParWorkers()
parseFun <- function(x) {
foo <- html_nodes(x, 'tr:nth-child(4) > td.tdtop1')
foo <- html_text(foo[1])
foo <- str_replace_all(foo, ",", "")
foo <- num(foo)
return(foo)
}
num_edits <- laply(edits_list[1:100], parseFun, .parallel = TRUE, .paropts = (.packages = c("rvest", "stringr", .export = "edits_list[1:100")))
load("../data/edits.RData")
## import article data -------------------
edits_names <- list.files("../data/wikipagesEditStatistics")
edits_list <- list()
for (i in 1:1000) {
edits_list[[i]] <- read_html(paste0("../data/wikipagesEditStatistics/", edits_names[i]))
if (i%%1000==0) { print(paste0(i, " imports"))}
}
num_edits <- laply(edits_list, parseFun, .parallel = TRUE, .paropts = (.packages = c("rvest", "stringr", .export = "edits_list")))
num_edits <- laply(edits_list, parseFun, .parallel = FALSE)
num_edits_np <- num_edits
num_edits <- laply(edits_list, parseFun, .parallel = TRUE, .paropts = (.packages = c("rvest", "stringr", .export = "edits_list")))
num_edits
parseFun <- function(x) {
foo <- html_nodes(x, 'tr:nth-child(4) > td.tdtop1')
foo <- html_text(foo)
foo <- str_replace_all(foo, ",", "")
foo <- num(foo)
return(foo)
}
num_edits <- laply(edits_list, parseFun, .parallel = TRUE, .paropts = (.packages = c("rvest", "stringr", .export = "edits_list")))
num_edits <- laply(edits_list, parseFun, .parallel = FALSE)
warnings()
summary(num_edits)
parseFun <- function(x) {
foo <- html_nodes(x, 'tr:nth-child(4) > td.tdtop1')
foo <- html_text(foo[1])
foo <- str_replace_all(foo, ",", "")
foo <- num(foo)
return(foo)
}
num_edits <- laply(edits_list, parseFun, .parallel = FALSE)
num_edits_np <- num_edits
num_edits <- laply(edits_list, parseFun, .parallel = TRUE, .paropts = (.packages = c("rvest", "stringr", .export = "edits_list")))
num_edits <- laply(edits_list, parseFun, .parallel = TRUE, .paropts = (.packages = c("rvest", "stringr")))
num_edits <- laply(edits_list, parseFun, .parallel = TRUE)
ptm <- proc.time()
num_edits <- laply(edits_list, parseFun, .parallel = TRUE)
diff1 <-  proc.time() - ptm diff1
ptm <- proc.time()
num_edits <- laply(edits_list, parseFun, .parallel = TRUE)
diff1 <-  proc.time() - ptm
ptm <- proc.time()
num_edits <- laply(edits_list, parseFun, .parallel = FALSE)
diff2 <-  proc.time() - ptm
diff1
diff2
num_edits <- laply(edits_list[1:1000], {. %>% html_nodes('tr:nth-child(4) > td.tdtop1') %>% extract(1) %>% html_text %>% str_replace_all(",", "") %>% num}, .parallel = TRUE)
setdiff(num_edits, num_edits_np)
detectCores()
registerDoMC(3)
?getDoParWorkers
getwd()
remove.packages("xml2")
install.packages("xml2")
library(xml2)
getwd()
setwd("/Users/simon/Dropbox/Uni/Forschung/2016 Measuring prominence of political elites/analysis")
stopCluster()
library(doMC)
stopCluster
stopCluster()
## load packages and functions -------------------------------
source("_packages.r")
source("_functions.r")
## load article names ----------------------------------------
load("../data/articlesHTML.RData")
registerDoMC(3)
parseEdits <- function(x) {
parsed_url <- read_html(x)
article_name <- basename(x) %>% str_replace(".html$", "")
num_edits <- numeric()
num_editors <- numeric()
num_extlinks <- numeric()
num_avgeditsyear <- numeric()
num_editslastyear <- numeric()
num_edits <- parsed_url %>% html_nodes('tr:nth-child(4) > td.tdtop1') %>% extract(1) %>% html_text %>% str_replace_all(",", "") %>% num
num_editors <- parsed_url %>% html_nodes('tr:nth-child(5) > td.tdtop1') %>% extract(1) %>% html_text %>% str_replace_all(",", "") %>% num
num_extlinks <- parsed_url %>% html_nodes('tr:nth-child(10) > td:nth-child(2) > span') %>% extract(1) %>% html_text %>% str_replace_all(",", "") %>% num
num_avgeditsyear <- parsed_url %>% html_nodes('tr:nth-child(14) > td.tdtop1') %>% extract(1) %>% html_text %>% str_replace_all(",", "") %>% num
num_editslastyear <- parsed_url %>% html_nodes('tr:nth-child(19) > td.tdtop1') %>% extract(1) %>% html_text %>% str_replace_all(",", "") %>% num
df_out <- data.frame(article_name, num_edits, num_editors, num_extlinks, num_avgeditsyear, num_editslastyear, stringsAsFactors = FALSE)
return(df_out)
}
edits_names <- list.files("../data/wikipagesEditStatistics", full.names = TRUE)
edits_df <- ldply(edits_names[1:10], parseEdits, .parallel = TRUE)
edits_df
head(edits_df)
sapply(edits_df, class)
edits_df <- ldply(edits_names, parseEdits, .progress = "text")
edits_df <- ldply(edits_names[1:10], parseEdits, .progress = "text")
library(microbenchmark)
microbenchmark(
edits_df <-   ldply(edits_names[1:10], parseEdits, .progress = "text")
)
microbenchmark()
?microbenchmark
microbenchmark(ldply(edits_names[1:10], parseEdits, .progress = "text"), times = 3)
(47000/10)/3600
setwd("/Users/simon/GitHub/Web-scraping-with-R-Extended-Edition")
dir()
source("00-course-setup.r")
wd <- getwd()
tempwd <- ("data/breweriesGermany")
dir.create(tempwd)
setwd(tempwd)
url <- "http://www.biermap24.de/brauereiliste.php"
content <- read_html(url, encoding = "utf8")
content <- read_html(url
)
foo <- html_table(content)
foo <- html_table(content, fill = TRUE)
names(foo)
foo
foo[[3]]
tab <- foo[[3]]
View(tab)
url <- "http://www.biermap24.de/brauereiliste.php"
content <- read_html(url, encoding = "utf8")
content <- read_html(url)
anchors <- html_nodes(content, xpath = "//tr/td[2]")
cities <- html_text(anchors)
cities
cities <- str_trim(cities)
cities <- cities[str_detect(cities, "^[[:upper:]]+.")]
length(cities)
length(unique(cities))
sort(table(cities))
# geocoding takes a while -> save results
# 2500 requests allowed per day
if ( !file.exists("breweries_geo.RData")){
pos <- geocode(cities)
geocodeQueryCheck()
save(pos, file="breweries_geo.RData")
} else {
load("breweries_geo.RData")
}
head(pos)
dim(pos)
length(unique(cities))
brewery_map <- get_map(location=c(lon=mean(pos$lon), lat=mean(pos$lat)), zoom="auto", maptype="hybrid")
mean(pos$lon)
pos
names(pos)
sort(pos)
sort(pos$lon)
class(pos)
class(pos$lon)
mean(pos$lon)
mean(pos$lat)
summary(pos)
?complete.cases
complete.cases(pos)
pos <- pos[complete.cases(pos),]
save(pos, file="breweries_geo.RData")
table(complete.cases(pos))
brewery_map <- get_map(location=c(lon=mean(pos$lon), lat=mean(pos$lat)), zoom="auto", maptype="hybrid")
p <- ggmap(brewery_map) + geom_point(data=pos, aes(x=lon, y=lat), col="red", size=3)
p
brewery_map <- get_map(location=c(lon=mean(pos$lon), lat=mean(pos$lat)), zoom=2, maptype="hybrid")
p <- ggmap(brewery_map) + geom_point(data=pos, aes(x=lon, y=lat), col="red", size=3)
p
brewery_map <- get_map(location=c(lon=mean(pos$lon), lat=mean(pos$lat)), zoom=4, maptype="hybrid")
p <- ggmap(brewery_map) + geom_point(data=pos, aes(x=lon, y=lat), col="red", size=3)
p
brewery_map <- get_map(location=c(lon=mean(pos$lon), lat=mean(pos$lat)), zoom=6, maptype="hybrid")
p <- ggmap(brewery_map) + geom_point(data=pos, aes(x=lon, y=lat), col="red", size=2)
p
View(pos)
pos <- filter(pos, lat>45, lat < 55)
pos <- filter(pos, lon > 4, lon < 17)
save(pos, file="breweries_geo.RData")
brewery_map <- get_map(location=c(lon=mean(pos$lon), lat=mean(pos$lat)), zoom=6, maptype="hybrid")
p <- ggmap(brewery_map) + geom_point(data=pos, aes(x=lon, y=lat), col="red", size=2)
p
View(pos)
pos <- filter(pos, lat > 47.3, lat < 54.5, lon > 6, lon < 15, !is.na(lon))
save(pos, file="breweries_geo.RData")
dim(pos)
brewery_map <- get_map(location=c(lon=mean(pos$lon), lat=mean(pos$lat)), zoom=6, maptype="hybrid")
p <- ggmap(brewery_map) + geom_point(data=pos, aes(x=lon, y=lat), col="red", size=1)
p
brewery_map <- get_map(location=c(lon=mean(pos$lon), lat=mean(pos$lat)), zoom=3, maptype="hybrid")
p <- ggmap(brewery_map) + geom_point(data=pos, aes(x=lon, y=lat), col="red", size=1)
p
brewery_map <- get_map(location=c(lon=mean(pos$lon), lat=mean(pos$lat)), zoom=6, maptype="hybrid")
p <- ggmap(brewery_map) + geom_point(data=pos, aes(x=lon, y=lat), col="red", size=1)
p
wd <- getwd()
setwd(wd)
setwd("../../")
wd <- getwd()
tempwd <- ("data/wikipediaStatisticians")
dir.create(tempwd)
setwd(tempwd)
## step 1: inspect page
url <- "https://en.wikipedia.org/wiki/List_of_statisticians"
browseURL(url)
## step 2: retrieve links
html <- read_html(url)
anchors <- html_nodes(html, xpath = "//ul/li/a[1]")
links <- html_attr(anchors, "href")
links
links_iffer <-
seq_along(links) >=
seq_along(links)[str_detect(links, "Odd_Aalen")] &
seq_along(links) <=
seq_along(links)[str_detect(links, "George_Kingsley_Zipf")] &
str_detect(links, "/wiki/")
links_index <- seq_along(links)[links_iffer]
links <- links[links_iffer]
length(links)
##  step 3: extract names
names <- html_attr(anchors, "title")[links_index]
names <- str_replace(names, " \\(.*\\)", "")
## step 4: fetch personal wiki pages
baseurl <- "http://en.wikipedia.org"
HTML <- list()
Fname <- str_c(basename(links), ".html")
URL <- str_c(baseurl, links)
# loop
for ( i in seq_along(links) ){
# url
url <- URL[i]
# fname
fname <- Fname[i]
# download
if ( !file.exists(fname) ) download.file(url, fname)
# read in files
HTML[[i]] <- read_html(fname)
}
## step 5: identify links between statisticians
# loop preparation
connections <- data.frame(from=NULL, to=NULL)
# loop
for (i in seq_along(HTML)) {
pslinks <- html_attr(
html_nodes(HTML[[i]], xpath="//p//a"), # note: only look for links in p sections; otherwise too many links collected
"href")
links_in_pslinks <- seq_along(links)[links %in% pslinks]
links_in_pslinks <- links_in_pslinks[links_in_pslinks!=i]
connections <- rbind(
connections,
data.frame(
from=rep(i-1, length(links_in_pslinks)), # -1 for zero-indexing
to=links_in_pslinks-1 # here too
)
)
}
# results
names(connections) <- c("from", "to")
head(connections)
# make symmetrical
connections <- rbind(
connections,
data.frame(from=connections$to,
to=connections$from)
)
connections <- connections[!duplicated(connections),]
## step 6: visualize connections
connections$value <- 1
nodesDF <- data.frame(name = names, group = 1)
network_out <- forceNetwork(Links = connections, Nodes = nodesDF, Source = "from", Target = "to", Value = "value", NodeID = "name", Group = "group", zoom = TRUE, opacityNoHover = 3)
saveNetwork(network_out, file = 'connections.html')
browseURL("connections.html")
## step 7: identify top nodes in data frame
nodesDF$id <- as.numeric(rownames(nodesDF)) - 1
connections_df <- merge(connections, nodesDF, by.x = "to", by.y = "id", all = TRUE)
to_count_df <- count(connections_df, name)
arrange(to_count_df, desc(n))
browseURL("http://flukeout.github.io/") # let's play this together until plate 8 or so!
url <- "https://www.buzzfeed.com/?country=us"
url_parsed <- read_html(url)
class(url_parsed)
html_structure(url_parsed)
as_list(url_parsed)
headings_nodes <- html_nodes(url_parsed, css = ".lede__link")
headings <- html_text(headings_nodes)
headings <- str_replace_all(headings, "\\n", "") %>% str_trim()
headings
url <- "https://www.jstatsoft.org/about/editorialTeam"
url_parsed <- read_html(url)
names <- html_nodes(url_parsed, css = ".member a") %>% html_text()
names <- html_nodes(url_parsed, "#group a") %>% html_text()
affiliations <- html_nodes(url_parsed, ".member li") %>% html_text()
str_detect(affiliations, "tatisti|athemati") %>% table
url <- "https://en.wikipedia.org/wiki/Joint_Statistical_Meetings"
browseURL(url)
url <- "https://en.wikipedia.org/wiki/Joint_Statistical_Meetings"
browseURL(url)
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)
tables
meetings <- tables[[2]]
class(meetings)
head(meetings)
table(meetings$Location) %>% sort()
browseURL("https://en.wikipedia.org/wiki/List_of_tallest_buildings_in_the_world")
url <- "https://en.wikipedia.org/wiki/List_of_tallest_buildings_in_the_world"
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)
buildings <- tables[[7]]
table(buildings$`Country/region`) %>% sort
table(buildings$City) %>% sort
names(table(buildings$`Country/region`) )
names(buildings)
buildings <- tables[[6]]
table(buildings$`Country/region`) %>% sort
table(buildings$City) %>% sort
browseURL("http://selectorgadget.com/")
browseURL("https://www.buzzfeed.com/?country=us")
url <- "https://www.buzzfeed.com/?country=us"
url_parsed <- read_html(url)
authors <- html_nodes(url_parsed, css = ".small-meta__item:nth-child(1) a") %>% html_text()
table(authors) %>% sort
tempwd <- ("data/jstatsoftStats")
setwd(wd)
tempwd <- ("data/jstatsoftStats")
dir.create(tempwd)
setwd(tempwd)
browseURL("http://www.jstatsoft.org/")
min(pos$lon)
max(pos$lon)
brewery_map <- get_map(location=c(lon=mean(min(pos$lon), max(pos$lon)), lat=mean(min(pos$lat), max(post$lat)), zoom=6, maptype="hybrid")
brewery_map <- get_map(location=c(lon=mean(min(pos$lon), max(pos$lon)), lat=mean(min(pos$lat))), max(post$lat)), zoom=6, maptype="hybrid")
brewery_map <- get_map(location=c(lon = mean(min(pos$lon), max(pos$lon)), lat = mean(min(pos$lat), max(post$lat))), zoom=6, maptype="hybrid")
brewery_map <- get_map(location=c(lon = mean(min(pos$lon), max(pos$lon)), lat = mean(min(pos$lat), max(pos$lat))), zoom=6, maptype="hybrid")
p <- ggmap(brewery_map) + geom_point(data=pos, aes(x=lon, y=lat), col="red", size=1)
p
summary(pos$lon)
View(pos)
mean(min(pos$lon), max(pos$lon))
min(pos$lon), max(pos$lon)
c(min(pos$lon), max(pos$lon))
c(min(pos$lon), max(pos$lon)) %>% mean
mean(min(pos$lon), max(pos$lon))
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
p <- ggmap(brewery_map) + geom_point(data=pos, aes(x=lon, y=lat), col="red", size=1)
p
browseURL("http://www.jstatsoft.org/")
# construct list of urls
baseurl <- "http://www.jstatsoft.org/article/view/v"
volurl <- paste0("0", seq(1,73,1))
volurl[1:9] <- paste0("00", seq(1, 9, 1))
brurl <- paste0("0", seq(1,9,1))
urls_list <- paste0(baseurl, volurl)
urls_list <- paste0(rep(urls_list, each = 9), "i", brurl)
names <- paste0(rep(volurl, each = 9), "_", brurl, ".html")
# download pages
folder <- "html_articles/"
dir.create(folder)
for (i in 1:length(urls_list)) {
if (!file.exists(paste0(folder, names[i]))) {
download.file(urls_list[i], destfile = paste0(folder, "/", names[i]))
Sys.sleep(runif(1, 0, 1))
}
}
list_files <- list.files(folder, pattern = "0.*")
list_files_path <-  list.files(folder, pattern = "0.*", full.names = TRUE)
length(list_files)
# delete non-existing articles
files_size <- sapply(list_files_path, file.size)
table(files_size) %>% sort()
delete_files <- list_files_path[files_size == 24265]
sapply(delete_files, file.remove)
list_files_path <-  list.files(folder, pattern = "0.*", full.names = TRUE) # update list of files
# import pages and extract content
authors <- character()
title <- character()
statistics <- character()
numViews <- numeric()
datePublish <- character()
for (i in 1:length(list_files_path)) {
html_out <- read_html(list_files_path[i])
table_out <- html_table(html_out, fill = TRUE)[[6]]
authors[i] <- table_out[1,2]
title[i] <- table_out[2,2]
statistics[i] <- table_out[4,2]
numViews[i] <- statistics[i] %>% str_extract("[[:digit:]]+") %>% as.numeric()
datePublish[i] <- statistics[i] %>% str_extract("[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}.$") %>% str_replace("\\.", "")
}
# construct data frame
dat <- data.frame(authors = authors, title = title, numViews = numViews, datePublish = datePublish)
head(dat)
# download statistics
dattop <- dat[order(dat$numViews, decreasing = TRUE),]
dattop[1:10,]
summary(dat$numViews)
plot(density(dat$numViews, from = 0, ), yaxt="n", ylab="", xlab="Number of views", main="Distribution of article page views in JSTATSOFT")
getwd()
setwd("/Users/simon/GitHub/Web-scraping-with-R-Extended-Edition/")
url <- "http://www.iea.org/policiesandmeasures/renewableenergy/"
browseURL(url)
content <- read_html(url)
html_content(/Users/simon/GitHub/Web-scraping-with-R-Extended-Edition/)
html_content(content)
html_structure(content)
checkForServer()
vignette("RSelenium-docker", package = "RSelenium")
vignette("RSelenium-basics", package = "RSelenium")
startServer()
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
remDr$open()
remDr$navigate(url)
remDr$open()
remDr$navigate(url)
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
remDr$open()
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "chrome")
remDr$open()
?remoteDriver
remDr <- remoteDriver$new()
remDr$open()
forecast <- read_xml("http://export.arxiv.org/api/query?search_query=all:forecast")
xml_ns(forecast) # inspect namespaces
authors <- xml_find_all(forecast, "//d1:author", ns = xml_ns(forecast))
authors %>% xml_text()
library(aRxiv)
browseURL("http://ropensci.org/tutorials/arxiv_tutorial.html")
ls("package:aRxiv")
arxiv_df <- arxiv_search(query = "forecast AND submittedDate:[2016 TO 2017]", limit = 500, output_format = "data.frame")
View(arxiv_df)
arxiv_count('au:"Gary King"')
query_terms
arxiv_count('abs:"political" AND submittedDate:[2016 TO 2017]')
polsci_articles <- arxiv_search('abs:"political" AND submittedDate:[2016 TO 2017]', limit = 500)
library(pageviews)
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user", start = "2016010100", end = "20160729")
head(trump_views)
clinton_views <- article_pageviews(project = "en.wikipedia", article = "Hillary Clinton", user_type = "user", start = "2016010100", end = "20160729")
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user", start = "2016010100", end = "20161011")
head(trump_views)
clinton_views <- article_pageviews(project = "en.wikipedia", article = "Hillary Clinton", user_type = "user", start = "2016010100", end = "20161011")
plot(ymd_h(trump_views$timestamp), trump_views$views, col = "red", type = "l")
lines(ymd_h(clinton_views$timestamp), clinton_views$views, col = "blue")
title <- "Groundhog Day" %>% URLencode()
endpoint <- "http://www.omdbapi.com/?"
url <- paste0(endpoint, "t=", title, "&tomatoes=true")
omdb_res = GET(url)
res_list <- content(omdb_res, as =  "parsed")
res_list %>% unlist() %>% t() %>% data.frame(stringsAsFactors = FALSE)
url <- paste0(endpoint, "s=", title)
omdb_res = GET(url)
res_list <- content(omdb_res, as = "text") %>% jsonlite::fromJSON(flatten = TRUE)
res_list$Search
apikey <- "&appid=42c7829f663f87eb05d2f12ab11f2b5d"
endpoint <- "http://api.openweathermap.org/data/2.5/find?"
city <- "Chicago,IL"
metric <- "&units=metric"
url <- paste0(endpoint, "q=", city, metric, apikey)
weather_res <- GET(url)
res_list <- content(weather_res, as =  "parsed")
res_list <- content(weather_res, as =  "text")  %>% jsonlite::fromJSON(flatten = TRUE)
res_list$list
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
setup_twitter_oauth(api_key,api_secret)
tweets <- searchTwitter(searchString = "Trump", n=25)
tweets_df <- twListToDF(tweets)
head(tweets_df)
names(tweets_df)
load("../rscraping-intro-duke-2/twitter_auth.RData")
filterStream("tweets_stream.json", track = c("Trump"), timeout = 10, oauth = twitCred)
tweets <- parseTweets("tweets_stream.json", simplify = TRUE)
names(tweets)
cat(tweets$text[1])
